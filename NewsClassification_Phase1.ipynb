{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to times of india webpage\n",
    "sURL=\"https://timesofindia.indiatimes.com/\"\n",
    "driver=webdriver.Chrome(\"/Users/bhupendrakumar/Documents/Personal/chromedriver\")\n",
    "driver.get(sURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to archives page\n",
    "link=driver.find_element(By.LINK_TEXT,\"Archives\")\n",
    "Archives_Address=link.get_attribute('href')\n",
    "driver.get(Archives_Address)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data for Feb 2018\n",
    "links=driver.find_elements(By.LINK_TEXT,\"February\")\n",
    "\n",
    "from datetime import date\n",
    "d=date.today()\n",
    "ActiveYear=int(d.strftime(\"%Y\"))\n",
    "YearToBesearched=2018\n",
    "Link_Index=ActiveYear-YearToBesearched\n",
    "driver.get(links[Link_Index].get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store links of days\n",
    "Days_links=[]\n",
    "for iDay in range(1,29):\n",
    "    webaddress=driver.find_element(By.LINK_TEXT,str(iDay)).get_attribute('href')\n",
    "    Days_links.append(webaddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store news links\n",
    "News_links=[]\n",
    "for iDay in Days_links:\n",
    "    driver.get(iDay)\n",
    "    time.sleep(2)\n",
    "    #Set of news links\n",
    "    News_Addresses=driver.find_elements_by_xpath(\"//span[@style='font-family:arial ;font-size:12;color: #006699']\")\n",
    "    \n",
    "    #News links\n",
    "    Text_Links=News_Addresses[0].text\n",
    "    texts=Text_Links.split(\"\\n\")\n",
    "    pattern=r\"[a-zA-Z]+\"\n",
    "    for sText in texts:    \n",
    "        if re.search(pattern,sText):\n",
    "            webaddress=driver.find_element(By.LINK_TEXT,sText).get_attribute('href')\n",
    "            News_links.append(webaddress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and intializing lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Headlines_list=[]\n",
    "Authors_list=[]\n",
    "Dates_list=[]\n",
    "News_list=[]\n",
    "Verticals_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sNews_Link in News_links[5400:]:\n",
    "    driver.get(sNews_Link)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//div[@class='error']\")\n",
    "    except NoSuchElementException:\n",
    "        #Extracting Headlines\n",
    "        try:\n",
    "            headline=driver.find_element_by_xpath(\"//h1[@class='_23498']\")\n",
    "            headline=headline.text\n",
    "        except ElementNotInteractableException:\n",
    "            headline=driver.find_element_by_xpath(\"//h1[@class='heading1']\")\n",
    "            headline=headline.text\n",
    "        except NoSuchElementException:\n",
    "            try:\n",
    "                headline=driver.find_element_by_xpath(\"//h1[@class='heading1']\")\n",
    "                headline=headline.text\n",
    "            except NoSuchElementException:\n",
    "                headline=\"-\"\n",
    "        Headlines_list.append(headline)\n",
    "\n",
    "        #Extracting Authors and Dates\n",
    "        try:\n",
    "            Author_Date=driver.find_element_by_xpath(\"//div[@class='_3Mkg- byline']\").text\n",
    "            if \"|\" in Author_Date:\n",
    "                Authors_list.append(Author_Date[:Author_Date.index(\" | \")])\n",
    "            else:\n",
    "                Authors_list.append(\"-\")\n",
    "                \n",
    "            if \"Feb\" not in Author_Date:\n",
    "                Dates_list.append(\"-\")\n",
    "            else:\n",
    "                Dates_list.append(Author_Date[Author_Date.index(\"Feb\"):])\n",
    "                \n",
    "        except ElementNotInteractableException:\n",
    "            Author_Date=driver.find_element_by_xpath(\"//div[@class='as_byline']\")\n",
    "            Author_Date=Author_Date.text.split('\\n')[0].replace(\"Created:\",\"\")\n",
    "            Authors_list.append(Author_Date[Author_Date.index(\"-\")+2:Author_Date.index(\"Feb\")])\n",
    "            if \"Feb\" not in Author_Date:\n",
    "                Dates_list.append(\"-\")\n",
    "            else:\n",
    "                Dates_list.append(Author_Date[Author_Date.index(\"Feb\"):])\n",
    "        except NoSuchElementException:\n",
    "            try:\n",
    "                Author_Date=driver.find_element_by_xpath(\"//div[@class='as_byline']\")\n",
    "                Author_Date=Author_Date.text.split('\\n')[0].replace(\"Created:\",\"\")\n",
    "                Authors_list.append(Author_Date[Author_Date.index(\"-\")+2:Author_Date.index(\"Feb\")])\n",
    "                if \"Feb\" not in Author_Date:\n",
    "                    Dates_list.append(\"-\")\n",
    "                else:\n",
    "                    Dates_list.append(Author_Date[Author_Date.index(\"Feb\"):])\n",
    "            except NoSuchElementException:\n",
    "                Dates_list.append(\"-\")\n",
    "                Authors_list.append(\"-\")\n",
    "        #Extracting Description\n",
    "        try:\n",
    "            News=driver.find_element_by_xpath(\"//div[@class='ga-headlines']\").text\n",
    "        except ElementNotInteractableException:\n",
    "            News=driver.find_element_by_xpath(\"//div[@class='Normal']\").text\n",
    "            News=News.replace(\"\\n\",\"\")\n",
    "        except NoSuchElementException:\n",
    "            News=driver.find_element_by_xpath(\"//div[@class='Normal']\").text\n",
    "            News=News.replace(\"\\n\",\"\")\n",
    "        News_list.append(News)\n",
    "\n",
    "        #Extracting Verticals\n",
    "        try:\n",
    "            Vertical=driver.find_elements_by_xpath(\"//span[@itemprop='name']\")\n",
    "            if len(Vertical)>=2:\n",
    "                Vert=Vertical[1].text\n",
    "            else:\n",
    "                Vert=\"-\"\n",
    "        except ElementNotInteractableException:\n",
    "            Vert=\"-\"\n",
    "        except NoSuchElementException:\n",
    "            Vert=\"-\"\n",
    "        \n",
    "        Verticals_list.append(Vert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring the dataframe\n",
    "Scrapped_data=pd.DataFrame()\n",
    "\n",
    "#Initializing columns in the new data frame\n",
    "Scrapped_data['Date']=Dates_list\n",
    "Scrapped_data['Author']=Authors_list\n",
    "Scrapped_data['Vertical']=Verticals_list\n",
    "Scrapped_data['Headline']=Headlines_list\n",
    "Scrapped_data['Description']=News_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing data in csv file\n",
    "Scrapped_data.to_csv(\"/Users/bhupendrakumar/Documents/Personal/Internship/News Classification And Analysis/News Classification and Analysis Phase-1/ToI_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
